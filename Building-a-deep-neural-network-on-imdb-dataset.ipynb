{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <font color=darkgreen> Building a Deep Neural Network on IMDB dataset-Worksheet-v2.0 </font>\n\nThe IMDB dataset consists of 50,000 reviews from the Internet Movie Database split into 50% positive and negative reviews respectively. Also, 25,000 reviews are used for training while the remaining 25,000 for testing.","metadata":{}},{"cell_type":"markdown","source":"### <font color=deeppink> Import our essential libraries </font>","metadata":{}},{"cell_type":"code","source":"# We need os, sys, numpy, tensorflow, matplotlib.pyplot\nimport os\nimport sys\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:07:12.803730Z","iopub.execute_input":"2022-02-25T17:07:12.804212Z","iopub.status.idle":"2022-02-25T17:07:16.554052Z","shell.execute_reply.started":"2022-02-25T17:07:12.804100Z","shell.execute_reply":"2022-02-25T17:07:16.553284Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### <font color=deeppink> Loading the IMDB dataset </font>","metadata":{}},{"cell_type":"code","source":"# IMDB dataset comes pre-packaged with keras\nfrom tensorflow.keras.datasets import imdb\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n# # num_words=10000 will retain only the top 10,000 most frequently occurring words in the training data.\n# # this will discard rare words and help us stay focused on the vector data of manageable size.\n# # without this limitation, we would end up working with 88,585 unique words in the sample data","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:07:16.556549Z","iopub.execute_input":"2022-02-25T17:07:16.556738Z","iopub.status.idle":"2022-02-25T17:07:21.509270Z","shell.execute_reply.started":"2022-02-25T17:07:16.556713Z","shell.execute_reply":"2022-02-25T17:07:21.508462Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Display the dimensions of the data\n# Train data - axes, shape, datatype\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:07:21.510536Z","iopub.execute_input":"2022-02-25T17:07:21.510816Z","iopub.status.idle":"2022-02-25T17:07:21.514575Z","shell.execute_reply.started":"2022-02-25T17:07:21.510779Z","shell.execute_reply":"2022-02-25T17:07:21.513901Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Display the dimensions of the data\n# Test images - axes, shape, datatype\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:07:21.515859Z","iopub.execute_input":"2022-02-25T17:07:21.516671Z","iopub.status.idle":"2022-02-25T17:07:21.524108Z","shell.execute_reply.started":"2022-02-25T17:07:21.516631Z","shell.execute_reply":"2022-02-25T17:07:21.523377Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Display a sample review from the dataset\n# sample_review = \n# print(sample_review)\n# # Each word in the review has been mapped to an index","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:07:21.526578Z","iopub.execute_input":"2022-02-25T17:07:21.527572Z","iopub.status.idle":"2022-02-25T17:07:21.531713Z","shell.execute_reply.started":"2022-02-25T17:07:21.527534Z","shell.execute_reply":"2022-02-25T17:07:21.530865Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Let us print out the lengthiest review in the training data\n# lengthy_review = \n# print(lengthy_review)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:07:21.533021Z","iopub.execute_input":"2022-02-25T17:07:21.534267Z","iopub.status.idle":"2022-02-25T17:07:21.539486Z","shell.execute_reply.started":"2022-02-25T17:07:21.534238Z","shell.execute_reply":"2022-02-25T17:07:21.538863Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Now print out the number of words in the sample review and the lengthy review\n# print(f\"Number of words in the sample review = {}\",\n#       f\"\\nNumber of words in the lengthy review = {}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:07:21.540615Z","iopub.execute_input":"2022-02-25T17:07:21.541295Z","iopub.status.idle":"2022-02-25T17:07:21.546860Z","shell.execute_reply.started":"2022-02-25T17:07:21.541251Z","shell.execute_reply":"2022-02-25T17:07:21.546136Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Print the max index in the sample and lengthy review\n# print(f\"The maximum word index in the sample review = {}\",\n#      f\"\\nThe maximum word index in the lengthy review = {}\")\n\n# the index will never exceed 9999 since we are looking at the top 10,000 most frequently occurring words in the reviews","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:07:21.549320Z","iopub.execute_input":"2022-02-25T17:07:21.550152Z","iopub.status.idle":"2022-02-25T17:07:21.556476Z","shell.execute_reply.started":"2022-02-25T17:07:21.550114Z","shell.execute_reply":"2022-02-25T17:07:21.555744Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Now repeat loading the dataset into train_data2 and test_data2 and check for the max word index in the lengthiest review.\n# Here, we do not restrict to the maximum number of frequently occurring words\n(train_data2, train_labels2), (test_data2, test_labels2) = imdb.load_data()\nprint(train_data2.shape)\nprint()\n# Question: How many total words are there across all the reviews?\n# Answer: ","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:07:21.558321Z","iopub.execute_input":"2022-02-25T17:07:21.558879Z","iopub.status.idle":"2022-02-25T17:07:26.773422Z","shell.execute_reply.started":"2022-02-25T17:07:21.558819Z","shell.execute_reply":"2022-02-25T17:07:26.772559Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# View the worded review (print these statements one at a time, comment out the rest)\nword_index = imdb.get_word_index()\nprint(word_index)\n# The key is the word and the index is the value, hence to get back the word, we need to reverse its order\n# reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n# print(reverse_word_index)\n# sample_worded_review = ' '.join([reverse_word_index.get(i - 3, '-') for i in sample_review])\n# print(sample_worded_review)\n# lengthy_worded_review = ' '.join([reverse_word_index.get(i - 3 , '?') for i in lengthy_review])\n# print(lengthy_worded_review)\n# The indices are offset by 3 places because index 0 is reserved for \"padding\", \n# 1 for \"start of the sequence\" and 2 for 'unknown\"\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:07:26.774643Z","iopub.execute_input":"2022-02-25T17:07:26.775286Z","iopub.status.idle":"2022-02-25T17:07:26.871270Z","shell.execute_reply.started":"2022-02-25T17:07:26.775242Z","shell.execute_reply":"2022-02-25T17:07:26.867091Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### <font color=deeppink>Dataset preparation </font>\n\nIn this case, we cannot directly feed a list of integers, so we shall one-hot encode (multi-hot encoding) the data\nOne-hot encoding transforms the list into a vector where only the value of the index where the word occurs would be 1 and rest all 0. \nFor example: If we have a review as $[0, 34, 500, 45, 23]$, then one-hot encoding will create a 10,000 dimensional vector (10,000 values) with $1$ only in positions $0, 34, 500, 45, 23$ and the remaining 9,995 positions with values $0$.","metadata":{}},{"cell_type":"code","source":"# Prepare the dataset for the neural network, we need tensor values - normalized values between 0 to 1\ndef one_hot_encode_data(review, dimension=10000): \n    encoded_data = np.zeros((len(review), dimension))\n    for idx, words in enumerate(review):\n        encoded_data[idx, words] = 1. # 1. for a floating point value\n    return encoded_data\n\nx_train = one_hot_encode_data(train_data)\nx_test = one_hot_encode_data(test_data)\n\nprint(x_train.shape)\nprint(x_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:07:26.872559Z","iopub.execute_input":"2022-02-25T17:07:26.872851Z","iopub.status.idle":"2022-02-25T17:07:29.883749Z","shell.execute_reply.started":"2022-02-25T17:07:26.872801Z","shell.execute_reply":"2022-02-25T17:07:29.882756Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"x_train[0]","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:07:29.885455Z","iopub.execute_input":"2022-02-25T17:07:29.885734Z","iopub.status.idle":"2022-02-25T17:07:29.897400Z","shell.execute_reply.started":"2022-02-25T17:07:29.885695Z","shell.execute_reply":"2022-02-25T17:07:29.896501Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Convert the labels to numpy arrays and float values like the data\ny_train = np.asarray(train_labels).astype('float32')\ny_test = np.asarray(test_labels).astype('float32')\n\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:07:29.898644Z","iopub.execute_input":"2022-02-25T17:07:29.899288Z","iopub.status.idle":"2022-02-25T17:07:29.907517Z","shell.execute_reply.started":"2022-02-25T17:07:29.899242Z","shell.execute_reply":"2022-02-25T17:07:29.906569Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"y_train[0]","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:07:29.912365Z","iopub.execute_input":"2022-02-25T17:07:29.913002Z","iopub.status.idle":"2022-02-25T17:07:29.921264Z","shell.execute_reply.started":"2022-02-25T17:07:29.912968Z","shell.execute_reply":"2022-02-25T17:07:29.920397Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### <font color=deeppink> Building the model </font>","metadata":{}},{"cell_type":"code","source":"# Build the model -- do you recollect?\n# We need a Sequential model with 3 Dense layers, with 16 neurons in the first two layers\n# and the last with ____ neurons (num_classes) \n\nmodel = tf.keras.models.Sequential([\ntf.keras.layers.Dense(16, activation='relu', name='Dense1', input_shape=(10000,)),\ntf.keras.layers.Dense(16, activation='relu', name='Dense2'),\ntf.keras.layers.Dense(1, activation='sigmoid', name='Dense_final')\n])\n# Later, experiment with 2 Dense layers and 4 Dense layers also vary the hidden neurons (dense layer neurons)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:07:29.922707Z","iopub.execute_input":"2022-02-25T17:07:29.923412Z","iopub.status.idle":"2022-02-25T17:07:32.013279Z","shell.execute_reply.started":"2022-02-25T17:07:29.923371Z","shell.execute_reply":"2022-02-25T17:07:32.012496Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model.summary() # we need to mention the input_shape when defining the model in order to print the summary\n# What would be the input_shape?","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-25T17:07:32.014443Z","iopub.execute_input":"2022-02-25T17:07:32.015151Z","iopub.status.idle":"2022-02-25T17:07:32.024441Z","shell.execute_reply.started":"2022-02-25T17:07:32.015108Z","shell.execute_reply":"2022-02-25T17:07:32.023704Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### <font color=deeppink> Compiling the model </font>","metadata":{}},{"cell_type":"code","source":"# Compile the model\n# Here we shall specify the optimizer, the loss function and the metrics the model needs to focus on\nmodel.compile(optimizer='rmsprop',\nloss='binary_crossentropy',\nmetrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:07:32.025614Z","iopub.execute_input":"2022-02-25T17:07:32.026318Z","iopub.status.idle":"2022-02-25T17:07:32.039170Z","shell.execute_reply.started":"2022-02-25T17:07:32.026281Z","shell.execute_reply":"2022-02-25T17:07:32.038375Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### <font color=deeppink> Training the model </font>","metadata":{}},{"cell_type":"code","source":"# Let us prepare our validation dataset (10,000 reviews from the train dataset)\nx_val = x_train[:10000] # reserving the first 10,000 reviews for validation\nx_train_small = x_train[10000:] # reserving the remaining 15,000 reviews for training\ny_val = y_train[:10000]\ny_train_small = y_train[10000:]\n# Finally train the model on the train_data and train_labels, epochs=20 and batch_size=512\nhistory = model.fit(x_train_small, y_train_small,\nepochs=20,\nbatch_size=512,\nvalidation_data=(x_val, y_val))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-25T17:07:32.040616Z","iopub.execute_input":"2022-02-25T17:07:32.041278Z","iopub.status.idle":"2022-02-25T17:07:54.834401Z","shell.execute_reply.started":"2022-02-25T17:07:32.041235Z","shell.execute_reply":"2022-02-25T17:07:54.833550Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### <font color=deeppink> Plotting the training vs. validation accuracy </font>","metadata":{}},{"cell_type":"code","source":"history_dict = history.history\nloss_value = history_dict['loss']\nval_loss_value = history_dict['val_loss']\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nepochs = range(1, len(loss_value) + 1)\nplt.plot(epochs, loss_value, 'b', label='Training Loss')\nplt.plot(epochs, val_loss_value, 'r', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n# plt.savefig('D:/SCIT/MBA-DSDA/Semester-II/Deep Learning-2123/Code Files/Output/imdb_dnn_loss.png')\nplt.show()\n\nplt.figure()\n\nplt.plot(epochs, acc, 'b', label='Training Accuracy')\nplt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n# plt.savefig('D:/SCIT/MBA-DSDA/Semester-II/Deep Learning-2123/Code Files/Output/imdb_dnn_acc.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:07:54.835883Z","iopub.execute_input":"2022-02-25T17:07:54.836142Z","iopub.status.idle":"2022-02-25T17:07:55.273436Z","shell.execute_reply.started":"2022-02-25T17:07:54.836106Z","shell.execute_reply":"2022-02-25T17:07:55.272707Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### <font color=deeppink> Let us predict and see </font>","metadata":{}},{"cell_type":"code","source":"# Let us predict and see\n# y_pred = model.predict_classes(np.expand_dims(x_test[4], axis=0))\n# print('Y_pred = {} and Y_true = {}'.format(y_pred, y_test[4]))\n\n# Try building the model by replacing the last Dense layer to have __ input neurons \n# when the activation function=softmax\n# What should happen to the loss function? Try with binary_crossentropy, does it work?\n# Now recall what you used for the MNIST dataset and check if that loss function works?\n\n# Uncomment and execute the following two lines of code \ny_pred = model.predict(np.expand_dims(x_test[4], axis=0))\nprint('Y_pred = {} and Y_true = {}'.format(np.argmax(y_pred), y_test[4])) # Try without np.argmax first","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:07:55.274749Z","iopub.execute_input":"2022-02-25T17:07:55.275020Z","iopub.status.idle":"2022-02-25T17:07:55.388214Z","shell.execute_reply.started":"2022-02-25T17:07:55.274989Z","shell.execute_reply":"2022-02-25T17:07:55.387520Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# It is time for some more theory! -- go back to the ppt","metadata":{}},{"cell_type":"markdown","source":"### <font color=deeppink> Re-train the model for 4 epochs and then predict the results </font>","metadata":{}},{"cell_type":"code","source":"# Before executing the following code, ensure to re-run the model and its compilation code \n# Try ommitting the above step and you'll see the model's accuracy explode\n# This time we train on the complete training set for the ideal number of epochs\nhistory2 = model.fit(x_train, y_train,\nepochs=14,\nbatch_size=512)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:07:55.389387Z","iopub.execute_input":"2022-02-25T17:07:55.389670Z","iopub.status.idle":"2022-02-25T17:08:05.305908Z","shell.execute_reply.started":"2022-02-25T17:07:55.389633Z","shell.execute_reply":"2022-02-25T17:08:05.305160Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# history_dict = history2.history\n# loss_value = history_dict['loss']\n# val_loss_value = history_dict['val_loss']\n# acc = history_dict['accuracy']\n# val_acc = history_dict['val_accuracy']\n# epochs = range(1, len(loss_value) + 1)\n# plt.plot(epochs, loss_value, 'b', label='Training Loss')\n# plt.plot(epochs, val_loss_value, 'r', label='Validation Loss')\n# plt.title('Training and Validation Loss')\n# plt.xlabel('Epochs')\n# plt.ylabel('Loss')\n# plt.legend()\n# # plt.savefig('D:/SCIT/MBA-DSDA/Semester-II/Deep Learning-2123/Code Files/Output/imdb_dnn_loss.png')\n# plt.show()\n\n# plt.figure()\n\n# plt.plot(epochs, acc, 'b', label='Training Accuracy')\n# plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n# plt.title('Training and Validation Accuracy')\n# plt.xlabel('Epochs')\n# plt.ylabel('Accuracy')\n# plt.legend()\n# # plt.savefig('D:/SCIT/MBA-DSDA/Semester-II/Deep Learning-2123/Code Files/Output/imdb_dnn_acc.png')\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:08:05.307322Z","iopub.execute_input":"2022-02-25T17:08:05.307602Z","iopub.status.idle":"2022-02-25T17:08:05.316526Z","shell.execute_reply.started":"2022-02-25T17:08:05.307563Z","shell.execute_reply":"2022-02-25T17:08:05.315802Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model\n\n(loss_model, acc_model) = model.evaluate(x_test, y_test)\n# Print the test loss and test accuracy obtained\nprint(f'Test Loss = {loss_model} and Test Accuracy = {acc_model}')\n# Predict and see\nresults = model.predict(x_test)\nresults","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:08:05.318049Z","iopub.execute_input":"2022-02-25T17:08:05.318682Z","iopub.status.idle":"2022-02-25T17:08:12.541454Z","shell.execute_reply.started":"2022-02-25T17:08:05.318641Z","shell.execute_reply":"2022-02-25T17:08:12.540699Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Display the confusion matrix\nfrom sklearn.metrics import log_loss, accuracy_score, confusion_matrix\n# confusion_matrix(test_data, prediction_data)\nresults2 = model.predict(x_test)\nresults2 = np.argmax(results2,axis=1)\n# confusion_matrix(y_test, np.argmax(results2))\nprint(results2)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:08:12.542995Z","iopub.execute_input":"2022-02-25T17:08:12.543291Z","iopub.status.idle":"2022-02-25T17:08:16.374160Z","shell.execute_reply.started":"2022-02-25T17:08:12.543252Z","shell.execute_reply":"2022-02-25T17:08:16.372648Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# confusion_matrix?","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:08:16.375500Z","iopub.execute_input":"2022-02-25T17:08:16.375808Z","iopub.status.idle":"2022-02-25T17:08:16.380473Z","shell.execute_reply.started":"2022-02-25T17:08:16.375770Z","shell.execute_reply":"2022-02-25T17:08:16.379756Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Now write a function to display a descriptive confusion matrix\ndef display_confusion_matrix(actual, pred):\n    (tn, fp), (fn, tp) = confusion_matrix(actual, pred)\n    print('-----------Confusion Matrix------------')\n    print(f'True Positives = {tp} \\t True Negatives = {tn}',\n         f'\\nFale Negatives = {fn} \\t False Positives = {fp}')\n    print('=======================================')\n    print(f'Accuracy = {(tp+tn) / (tp+fn+tn+fp)}')\n    \ndisplay_confusion_matrix(y_test, results2)\n\n# Homework, complete the function to display recall(TPR), precision and FPR","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:08:16.381965Z","iopub.execute_input":"2022-02-25T17:08:16.382450Z","iopub.status.idle":"2022-02-25T17:08:16.479981Z","shell.execute_reply.started":"2022-02-25T17:08:16.382413Z","shell.execute_reply":"2022-02-25T17:08:16.479153Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"#### How do we decide the threshold??","metadata":{}},{"cell_type":"markdown","source":"# It is time for some more theory! -- go back to the ppt","metadata":{}},{"cell_type":"code","source":"t = np.arange(0, 1, 0.1)\nprint(t)\nfpr = []\ntpr = []\nfor t1 in t:\n    print(f'--Threshold {t1}--')\n    pred = np.zeros((y_test.shape))\n    for idx, r in enumerate(results):\n        if r > t1:\n            pred[idx] = 1\n    print(pred)\n    display_confusion_matrix(y_test, pred)\n    (tn, fp), (fn, tp) = confusion_matrix(y_test, pred)\n    fpr1 = fp / (fp + tn)\n    tpr1 = tp / (tp + fn)\n    fpr.append(fpr1)\n    tpr.append(tpr1)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:08:16.481403Z","iopub.execute_input":"2022-02-25T17:08:16.481655Z","iopub.status.idle":"2022-02-25T17:08:18.750246Z","shell.execute_reply.started":"2022-02-25T17:08:16.481621Z","shell.execute_reply":"2022-02-25T17:08:18.749551Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"print(fpr)\nprint(tpr)\n\nprint('--ROC CURVE-----')\nplt.figure()\nplt.plot(fpr, tpr)\nplt.title('ROC Curve')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:08:18.751631Z","iopub.execute_input":"2022-02-25T17:08:18.751862Z","iopub.status.idle":"2022-02-25T17:08:18.956258Z","shell.execute_reply.started":"2022-02-25T17:08:18.751824Z","shell.execute_reply":"2022-02-25T17:08:18.955534Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Repeat the above exercise for training on 4 epochs with the following changes to the model\n# Build a Sequential model with 3 Dense layers, with 31 and 17 neurons each and\n# the last with 1 neuron (num_classes)\nmodel3 = tf.keras.models.Sequential([\ntf.keras.layers.Dense(31, activation='relu', name='Dense1', input_shape=(10000,)),\ntf.keras.layers.Dense(17, activation='relu', name='Dense2'),\ntf.keras.layers.Dense(17, activation='relu', name='Dense3'),\n# tf.keras.layers.Dropout(0.5, name='Dropout1'),\ntf.keras.layers.Dense(1, activation='sigmoid', name='Dense_final')\n])\nmodel3.summary()\nmodel3.compile(optimizer='rmsprop',\nloss='binary_crossentropy',\nmetrics=['accuracy'])\nprint('---Model Training----')\nmodel3.fit(x_train, y_train, epochs=4, batch_size=512)\nprint('---Model Evaluation----')\nmodel3.evaluate(x_test, y_test)\npred3 = model3.predict(x_test)\ndisplay_confusion_matrix(y_test, np.argmax(pred3,axis=1))","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:08:18.957484Z","iopub.execute_input":"2022-02-25T17:08:18.957762Z","iopub.status.idle":"2022-02-25T17:08:31.850392Z","shell.execute_reply.started":"2022-02-25T17:08:18.957711Z","shell.execute_reply":"2022-02-25T17:08:31.849525Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"pred3 = model3.predict(x_test)\ndisplay_confusion_matrix(y_test, np.argmax(pred3,axis=-1))","metadata":{"execution":{"iopub.status.busy":"2022-02-25T17:08:31.851564Z","iopub.execute_input":"2022-02-25T17:08:31.851856Z","iopub.status.idle":"2022-02-25T17:08:35.293029Z","shell.execute_reply.started":"2022-02-25T17:08:31.851805Z","shell.execute_reply":"2022-02-25T17:08:35.292223Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"### <font color=deeppink> Dealing with overfitting </font>\n#### Add Dropout layers ","metadata":{}},{"cell_type":"markdown","source":"# It is time for some more theory! -- go back to the ppt","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}